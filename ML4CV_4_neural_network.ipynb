{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML4CV_4_neural_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicholasBaraghini/Machine-Learning-for-Computer-Vision-LAB-Sessions/blob/main/ML4CV_4_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywawXTDXzSCN"
      },
      "source": [
        "We start with our usual imports and figure adjustments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iiy4LlS5xD7J"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from timeit import default_timer as timer\n",
        "from functools import partial\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12.0, 8.0)\n",
        "plt.rcParams['font.size'] = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwsd_5l0LPQI"
      },
      "source": [
        "Then we load CIFAR10, and we create the usual `Dataset`s and `DataLoader`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXNjkzQBzW6x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "306bc0da-4376-4de3-b6e2-a404fc88a4fd"
      },
      "source": [
        "tsfms = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda z: z.reshape(-1))]) \n",
        "train_ds = torchvision.datasets.CIFAR10(root=\"/data/\", train=True, transform=tsfms, download=True)\n",
        "test_ds = torchvision.datasets.CIFAR10(root=\"/data/\", train=False, transform=tsfms)\n",
        "\n",
        "classes = train_ds.classes\n",
        "n_classes = len(classes)\n",
        "n_features = len(train_ds[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9QK1XnpzZWx"
      },
      "source": [
        "splitted_datasets = torch.utils.data.random_split(train_ds, [45000, 5000])\n",
        "actual_train_subds = splitted_datasets[0]\n",
        "valid_subds = splitted_datasets[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSJRHzdOzbUC"
      },
      "source": [
        "small_actual_train_subds = torch.utils.data.Subset(actual_train_subds, range(500))\n",
        "small_valid_subds = torch.utils.data.Subset(valid_subds, range(100))\n",
        "small_test_subds = torch.utils.data.Subset(test_ds, range(100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLBTTtEpzfDR"
      },
      "source": [
        "batch_size = 256\n",
        "small_actual_train_dl = torch.utils.data.DataLoader(small_actual_train_subds, batch_size=batch_size, shuffle=True)\n",
        "small_valid_dl = torch.utils.data.DataLoader(small_valid_subds, batch_size=batch_size)\n",
        "small_test_dl = torch.utils.data.DataLoader(small_test_subds, batch_size=batch_size)\n",
        "actual_train_dl = torch.utils.data.DataLoader(actual_train_subds, batch_size=batch_size, shuffle=True)\n",
        "valid_dl = torch.utils.data.DataLoader(valid_subds, batch_size=batch_size)\n",
        "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLnPsp6DLbaP"
      },
      "source": [
        "We will then create our first Neural Network. One way to create Neural Networks in PyTorch is by subclassing `torch.nn.Module`. In this way, our model will inherit a lot of ready-to-use convinience functions (access to parameters for optimization, get/set parameters, ...). \n",
        "\n",
        "We only need to create the layers we will use in the `__init__` function and define the `forward` function that specifies how to apply them. \n",
        "\n",
        "Layers are in turn subclasses of `torch.nn.Module`. In our example, we will use only linear layers, i.e. Fully Connected (FC) layers. Our network will have at least two FC layers: `self.first`, mapping the flattened input image into the (first) hidden representation, and `self.last`, mapping the (last) hidden representation into the scores for the classes.\n",
        "\n",
        "To play with varying depths and activation functions, we will have two additional parameters:\n",
        "\n",
        "\n",
        "*   `n_additional_hidden_layers`, specifies how many hidden layers our network has, beside `self.first`\n",
        "*   `use_relu`, if `False`, activations will be sigmoid functions, ReLUs otherwise\n",
        "\n",
        "Note that to store a variable number of layers in our network, we do not use plain PyTorch lists, but `torch.nn.ModuleList`. This is important to make PyTorch aware of the layers in the list, e.g. to set/get their parameters when calling the methods of the base `Module` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nbo9Igi2Lr5"
      },
      "source": [
        "class TwoPlusLayersNetwork(torch.nn.Module):\n",
        "  def __init__(self, n_features, hidden_width, n_classes, n_additional_hidden_layers=0, use_relu=True):\n",
        "    super(TwoPlusLayersNetwork, self).__init__()\n",
        "    self.first = torch.nn.Linear(n_features, hidden_width) \n",
        "    self.activation = torch.relu if use_relu else torch.sigmoid\n",
        "    self.last = torch.nn.Linear(hidden_width, n_classes)\n",
        "\n",
        "    self.additional_hidden_layers = torch.nn.ModuleList(\n",
        "        [torch.nn.Linear(hidden_width, hidden_width) for i in range(n_additional_hidden_layers)])\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.first.forward(x)\n",
        "    x = self.activation(x)\n",
        "    for layer in self.additional_hidden_layers:\n",
        "      x = layer.forward(x)\n",
        "      x = self.activation(x)\n",
        "    x = self.last.forward(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izRXo7zLOYQk"
      },
      "source": [
        "We then define the usual function to train a model.\n",
        "\n",
        "Note that we use \n",
        "*   `nn.parameters()` to get a list of trainable parameters\n",
        "*   `nn.state_dict()` to get the model parameters when we achieve better validation accuracy and save them in the `best_params` variable. \n",
        "\n",
        "These are two of the convinience functions our network inherits from `torch.nn.Module`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VddQeQtZ2zcv"
      },
      "source": [
        "def ncorrect(scores, y):\n",
        "  y_hat = torch.argmax(scores, 1)\n",
        "  return (y_hat==y).sum()\n",
        "\n",
        "def accuracy(scores, y):\n",
        "  correct = ncorrect(scores, y)\n",
        "  return correct.true_divide(y.shape[0])\n",
        "  \n",
        "def train_loop(n_features, hidden_width, n_classes, n_additional_hidden_layers, use_relu,\n",
        "               train_dl, epochs, partial_opt, \n",
        "               valid_dl=None, verbose=False):\n",
        "  best_valid_acc = 0\n",
        "  best_params = []\n",
        "  best_epoch = -1\n",
        "\n",
        "  nn = TwoPlusLayersNetwork(n_features, hidden_width, n_classes, n_additional_hidden_layers, use_relu)\n",
        "\n",
        "  # We \"complete\" the partial function by calling it and specifying the missing parameters\n",
        "  opt = partial_opt(nn.parameters())\n",
        "\n",
        "  for e in range(epochs):\n",
        "    #train\n",
        "    train_loss = 0\n",
        "    train_samples = 0\n",
        "    train_acc = 0\n",
        "    for train_data in train_dl:\n",
        "      scores = nn.forward(train_data[0])\n",
        "      loss = F.cross_entropy(scores, train_data[1])\n",
        "      train_loss += loss.item() * train_data[0].shape[0]\n",
        "      train_samples += train_data[0].shape[0]\n",
        "      train_acc += ncorrect(scores, train_data[1]).item()\n",
        "      loss.backward()\n",
        "\n",
        "      opt.step()\n",
        "      opt.zero_grad()\n",
        "\n",
        "    train_acc /= train_samples\n",
        "    train_loss /= train_samples\n",
        "    \n",
        "    # validation\n",
        "    with torch.no_grad():\n",
        "      valid_loss = 0\n",
        "      valid_samples = 0\n",
        "      valid_acc = 0\n",
        "      if valid_dl is not None:\n",
        "        for valid_data in valid_dl:\n",
        "          valid_scores = nn.forward(valid_data[0])\n",
        "          valid_loss += F.cross_entropy(valid_scores, valid_data[1]).item() * valid_data[0].shape[0]\n",
        "          valid_samples += valid_data[0].shape[0]\n",
        "          valid_acc += ncorrect(valid_scores, valid_data[1]).item()\n",
        "        valid_acc /= valid_samples\n",
        "        valid_loss /= valid_samples\n",
        "      \n",
        "      if valid_dl is None or valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc if valid_dl is not None else 0\n",
        "        best_params = nn.state_dict()\n",
        "        best_epoch = e\n",
        "\n",
        "      \n",
        "    if verbose and e % 10 == 0:\n",
        "      print(f\"Epoch {e}: train loss {train_loss:.3f} - train acc {train_acc:.3f}\" + (\"\" if valid_dl is None else f\" - valid loss {valid_loss:.3f} - valid acc {valid_acc:.3f}\"))\n",
        "  \n",
        "  if verbose and valid_dl is not None:\n",
        "    print(f\"Best epoch {best_epoch}, best acc {best_valid_acc}\")\n",
        "\n",
        "  return best_valid_acc, best_params, best_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pupldmv9ZaCe"
      },
      "source": [
        "The two functions are similar, but serve different purposes. \n",
        "\n",
        "`parameters()` returns a list (actually, a generator) of trainable tensors, which is what `Optimizer`s require: they do not need to know which tensor correspond to which layer, since they are all treated the same when performing SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4xBfqCeX5Ys",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "87f7443d-d735-4541-ca00-77a0b1417301"
      },
      "source": [
        "nn = TwoPlusLayersNetwork(n_features, hidden_width, n_classes, n_additional_hidden_layers=1, use_relu=True)\n",
        "for p in nn.parameters():\n",
        "  print(type(p), p.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.nn.parameter.Parameter'> None torch.Size([50, 3072])\n",
            "<class 'torch.nn.parameter.Parameter'> None torch.Size([50])\n",
            "<class 'torch.nn.parameter.Parameter'> None torch.Size([10, 50])\n",
            "<class 'torch.nn.parameter.Parameter'> None torch.Size([10])\n",
            "<class 'torch.nn.parameter.Parameter'> None torch.Size([50, 50])\n",
            "<class 'torch.nn.parameter.Parameter'> None torch.Size([50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNmyH3BfaFQo"
      },
      "source": [
        "`state_dict()` instead is an (Ordered) Dictionary, which associates each variable storing a layer in our classes with its parameters. It is therefore useful to obtain a snapshot of the parameters of our model that can later be restored by calling `load_state_dict()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW0_jwzAYUFB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be24fb1d-a172-4027-fec5-2b5e5d830b83"
      },
      "source": [
        "type(nn.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "collections.OrderedDict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEIEOMH3YhyS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7fb7fc4f-54a4-4fec-d6ef-84552ca7a452"
      },
      "source": [
        "nn.state_dict().keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['first.weight', 'first.bias', 'last.weight', 'last.bias', 'additional_hidden_layers.0.weight', 'additional_hidden_layers.0.bias'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j1uORe6YpBJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "45933120-9a4b-4de1-8bc0-41796455e67b"
      },
      "source": [
        "nn.state_dict()[\"first.bias\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0141,  0.0104,  0.0093,  0.0019,  0.0071,  0.0129,  0.0131, -0.0018,\n",
              "        -0.0164,  0.0076,  0.0072, -0.0075, -0.0068,  0.0118,  0.0068,  0.0172,\n",
              "         0.0024, -0.0086, -0.0102,  0.0095, -0.0133,  0.0001,  0.0098,  0.0133,\n",
              "         0.0033, -0.0034, -0.0120, -0.0093, -0.0103,  0.0037, -0.0024,  0.0179,\n",
              "         0.0114, -0.0150, -0.0080, -0.0055, -0.0069, -0.0176, -0.0028, -0.0049,\n",
              "         0.0139,  0.0176,  0.0077,  0.0115,  0.0174,  0.0032, -0.0127, -0.0115,\n",
              "        -0.0057,  0.0029])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_gDTTNKUOHS"
      },
      "source": [
        "Let's verify that the use of the sigmoid as activation function makes it more difficult to train \"deep\" networks, i.e. with 10 hidden layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s2xnpo73w5Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "8672d29f-6767-4061-8dd0-4a1060f09fa5"
      },
      "source": [
        "start = timer()\n",
        "lr=1e-3\n",
        "hidden_width = 50\n",
        "n_additional_hidden_layers = 10\n",
        "use_relu = False\n",
        "p_opt = partial(torch.optim.Adam, lr=lr)\n",
        "\n",
        "train_loop(n_features, hidden_width, n_classes, n_additional_hidden_layers, use_relu,\n",
        "           train_dl=small_actual_train_dl, epochs=200, partial_opt=p_opt, \n",
        "           valid_dl=small_valid_dl, verbose=True)\n",
        "end = timer()\n",
        "print(f\"Elapsed time (s): {end-start}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: train loss 2.333 - train acc 0.102 - valid loss 2.345 - valid acc 0.060\n",
            "Epoch 10: train loss 2.296 - train acc 0.124 - valid loss 2.294 - valid acc 0.160\n",
            "Epoch 20: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 30: train loss 2.294 - train acc 0.124 - valid loss 2.297 - valid acc 0.160\n",
            "Epoch 40: train loss 2.294 - train acc 0.124 - valid loss 2.296 - valid acc 0.160\n",
            "Epoch 50: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 60: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 70: train loss 2.295 - train acc 0.124 - valid loss 2.297 - valid acc 0.160\n",
            "Epoch 80: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 90: train loss 2.294 - train acc 0.124 - valid loss 2.297 - valid acc 0.160\n",
            "Epoch 100: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 110: train loss 2.294 - train acc 0.124 - valid loss 2.297 - valid acc 0.160\n",
            "Epoch 120: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 130: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 140: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 150: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 160: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 170: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Epoch 180: train loss 2.294 - train acc 0.124 - valid loss 2.297 - valid acc 0.160\n",
            "Epoch 190: train loss 2.294 - train acc 0.124 - valid loss 2.298 - valid acc 0.160\n",
            "Best epoch 4, best acc 0.16\n",
            "Elapsed time (s): 16.367571692999718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FZt1ciUUnue"
      },
      "source": [
        "Let's compare this with ReLU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTmDU9CZjD9G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "6169c878-f0f5-4155-db6a-22fa1ef42eb0"
      },
      "source": [
        "start = timer()\n",
        "lr=1e-3\n",
        "hidden_width = 50\n",
        "n_additional_hidden_layers = 10\n",
        "use_relu = True\n",
        "p_opt = partial(torch.optim.Adam, lr=lr)\n",
        "\n",
        "train_loop(n_features, hidden_width, n_classes, n_additional_hidden_layers, use_relu,\n",
        "           train_dl=small_actual_train_dl, epochs=200, partial_opt=p_opt, \n",
        "           valid_dl=small_valid_dl, verbose=True)\n",
        "end = timer()\n",
        "print(f\"Elapsed time (s): {end-start}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: train loss 2.312 - train acc 0.092 - valid loss 2.302 - valid acc 0.060\n",
            "Epoch 10: train loss 2.298 - train acc 0.124 - valid loss 2.293 - valid acc 0.160\n",
            "Epoch 20: train loss 2.236 - train acc 0.160 - valid loss 2.224 - valid acc 0.210\n",
            "Epoch 30: train loss 2.163 - train acc 0.190 - valid loss 2.153 - valid acc 0.230\n",
            "Epoch 40: train loss 2.090 - train acc 0.198 - valid loss 2.095 - valid acc 0.260\n",
            "Epoch 50: train loss 2.030 - train acc 0.230 - valid loss 2.070 - valid acc 0.230\n",
            "Epoch 60: train loss 1.973 - train acc 0.250 - valid loss 2.053 - valid acc 0.240\n",
            "Epoch 70: train loss 1.875 - train acc 0.282 - valid loss 2.050 - valid acc 0.220\n",
            "Epoch 80: train loss 1.836 - train acc 0.294 - valid loss 2.027 - valid acc 0.290\n",
            "Epoch 90: train loss 1.718 - train acc 0.340 - valid loss 2.023 - valid acc 0.320\n",
            "Epoch 100: train loss 1.687 - train acc 0.340 - valid loss 2.075 - valid acc 0.280\n",
            "Epoch 110: train loss 1.620 - train acc 0.356 - valid loss 2.089 - valid acc 0.320\n",
            "Epoch 120: train loss 1.544 - train acc 0.390 - valid loss 2.170 - valid acc 0.310\n",
            "Epoch 130: train loss 1.507 - train acc 0.410 - valid loss 2.295 - valid acc 0.310\n",
            "Epoch 140: train loss 1.454 - train acc 0.404 - valid loss 2.253 - valid acc 0.320\n",
            "Epoch 150: train loss 1.407 - train acc 0.440 - valid loss 2.466 - valid acc 0.330\n",
            "Epoch 160: train loss 1.317 - train acc 0.480 - valid loss 2.574 - valid acc 0.320\n",
            "Epoch 170: train loss 1.270 - train acc 0.486 - valid loss 2.565 - valid acc 0.290\n",
            "Epoch 180: train loss 1.342 - train acc 0.430 - valid loss 2.816 - valid acc 0.350\n",
            "Epoch 190: train loss 1.140 - train acc 0.546 - valid loss 2.832 - valid acc 0.300\n",
            "Best epoch 86, best acc 0.36\n",
            "Elapsed time (s): 16.546173849999832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elUlC9zcUvQq"
      },
      "source": [
        "You can see that training of modestly deep networks (for today standards) with the sigmoid function is stuck, while the network using ReLUs increases its performance while training.\n",
        "\n",
        "Let's then define a function to perform hyper-parameter tuning. Since this is a small network we can afford to validate also hyper-parameters defining the architecture, like the `hidden_width` of the layers, or the number of hidden layers. We will also run a loop over optimizers (wrapping learning rates), as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKG3pZ-532ge"
      },
      "source": [
        "def hyperparameter_tuning(n_features, n_classes, train_dl,  \n",
        "                          valid_dl, partial_opts, hidden_widths,\n",
        "                          n_additional_hidden_layers_list, epochs=5):\n",
        "  \n",
        "  best_valid_acc = 0\n",
        "  best_params = []\n",
        "  best_hyper_params = []\n",
        "\n",
        "  for hidden_width in hidden_widths:\n",
        "    for n_additional_hidden_layers in n_additional_hidden_layers_list:\n",
        "      for partial_opt in partial_opts:\n",
        "        run_valid_acc, params, epoch = train_loop(n_features, hidden_width, n_classes, n_additional_hidden_layers, use_relu=True, \n",
        "                  train_dl=train_dl, epochs=epochs, partial_opt=partial_opt, valid_dl=valid_dl, verbose=False)\n",
        "\n",
        "        if run_valid_acc > best_valid_acc:\n",
        "          best_valid_acc = run_valid_acc\n",
        "          best_params = params\n",
        "          best_hyper_params = [partial_opt, epoch, hidden_width, n_additional_hidden_layers]\n",
        "          print(f\"Improved result: acc {best_valid_acc:.3f}, best_hyper_params {best_hyper_params}\")\n",
        "  return best_hyper_params, best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY_AKgDwVnzC"
      },
      "source": [
        "Then, the usual function to define which combination of optimizers and learning rates we want to validate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5vwlbgDHZ6C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bc3c4074-12b9-49b0-b48c-6a536c49f293"
      },
      "source": [
        "def build_optlist():\n",
        "  lrs = [1e-4, 1e-3]\n",
        "  betas = [0.9]\n",
        "  opts = []\n",
        "  #opts += [partial(torch.optim.SGD, lr=lr) for lr in lrs]\n",
        "  #opts += [partial(torch.optim.SGD, lr=lr, momentum=beta, nesterov=True) for lr in lrs for beta in betas]\n",
        "  opts += [partial(torch.optim.Adam, lr=lr) for lr in lrs]\n",
        "  #opts += [partial(torch.optim.RMSprop, lr=lr) for lr in lrs]\n",
        "  return opts\n",
        "\n",
        "build_optlist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.0001),\n",
              " functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.001)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7CinT4dVx5a"
      },
      "source": [
        "Let' check everything works on the small `Dataset`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJIwGd6uIUkQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "eff9c97b-16ff-4ed0-e6fc-a6d0a2000b6c"
      },
      "source": [
        "start=timer()\n",
        "opts = build_optlist()\n",
        "hidden_widths = [128]\n",
        "n_hidden_layers_list = [0]\n",
        "best_hyper_params, best_params = hyperparameter_tuning(n_features, n_classes, small_actual_train_dl, \n",
        "                  small_valid_dl, opts, hidden_widths, n_hidden_layers_list, epochs=200)\n",
        "end=timer()\n",
        "print(f\"Elapsed time (s): {end-start:.3f}\")\n",
        "print(f\"best optimizer {best_hyper_params[0]}, best epoch {best_hyper_params[1]},\" \n",
        "      f\"best hidden_width {best_hyper_params[2]}, best n_additional_hidden {best_hyper_params[3]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Improved result: acc 0.340, best_hyper_params [functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.01), 62, 128, 0]\n",
            "Improved result: acc 0.470, best_hyper_params [functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.001), 25, 128, 0]\n",
            "Elapsed time (s): 34.131\n",
            "best optimizer functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.001), best epoch 25,best hidden_width 128, best n_additional_hidden 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1JYpRlnV4Xi"
      },
      "source": [
        "And then, let's validate on the real `Dataset`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41iJn4dAJRvY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "ca6132a0-76cd-4fa0-86c6-6d64a578f89a"
      },
      "source": [
        "start=timer()\n",
        "opts = build_optlist()\n",
        "hidden_widths = [128]\n",
        "n_hidden_layers_list = [0,1]\n",
        "best_hyper_params, best_params = hyperparameter_tuning(n_features, n_classes, \n",
        "      actual_train_dl, valid_dl, opts, hidden_widths, n_hidden_layers_list, epochs=30)\n",
        "end=timer()\n",
        "print(f\"Elapsed time (s): {end-start:.3f}\")\n",
        "print(f\"best optimizer {best_hyper_params[0]}, best epoch {best_hyper_params[1]}, \"\n",
        "      f\"best hidden_width {best_hyper_params[2]}, best n_additional_hidden {best_hyper_params[3]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Improved result: acc 0.253, best_hyper_params [functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.01), 20, 128, 0]\n",
            "Improved result: acc 0.494, best_hyper_params [functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.001), 21, 128, 0]\n",
            "Improved result: acc 0.505, best_hyper_params [functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.001), 29, 128, 1]\n",
            "Elapsed time (s): 932.106\n",
            "best optimizer functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.001), best epoch 29, best hidden_width 128, best n_additional_hidden 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJdDrnimWBrL"
      },
      "source": [
        "Let's train on the full training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb6B3-IoLvUL"
      },
      "source": [
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l6RNMV7LwBV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "72853dc2-1e79-4057-a5e8-7200ad2d3905"
      },
      "source": [
        "start = timer()\n",
        "best_opt = best_hyper_params[0]\n",
        "best_epochs = best_hyper_params[1]\n",
        "_, best_params, best_epoch = train_loop(n_features=n_features, \n",
        "        hidden_width=best_hyper_params[2], n_classes=n_classes, \n",
        "        n_additional_hidden_layers=best_hyper_params[3], use_relu=True,\n",
        "        train_dl=train_dl, epochs=best_epochs, partial_opt=best_opt, verbose=True)\n",
        "end = timer()\n",
        "print(f\"Elapsed time (s): {end-start}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: train loss 1.906 - train acc 0.314\n",
            "Epoch 10: train loss 1.389 - train acc 0.506\n",
            "Epoch 20: train loss 1.249 - train acc 0.556\n",
            "Elapsed time (s): 225.143524608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrn0gnLGWLAJ"
      },
      "source": [
        "And test on the full test set. To restore the parameters computed in training, we use the `load_state_dict` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weI1-WB2MBxN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c18ad925-cd99-4c44-b0cf-5db0bc9f9e41"
      },
      "source": [
        "nn = TwoPlusLayersNetwork(n_features, best_hyper_params[2], n_classes, best_hyper_params[3])\n",
        "nn.load_state_dict(best_params)\n",
        "\n",
        "start = timer()\n",
        "test_samples = 0\n",
        "test_acc = 0\n",
        "for test_data in test_dl:\n",
        "  test_scores = nn.forward(test_data[0])\n",
        "  test_samples += test_data[0].shape[0]\n",
        "  test_acc += ncorrect(test_scores, test_data[1]).item()\n",
        "test_acc /= test_samples\n",
        "end = timer()\n",
        "print(f\"Accuracy on full test set {test_acc:.3f}, elapsed time (s): {end-start:.3f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on full test set 0.512, elapsed time (s): 1.179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J03uNx8HVazi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}